---
title: "Combustível"
subtitle: "Uma análise estatística dos preços no Brasil"
author: "Icaro Pinheiro"
date: "09/05/2022"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

## Objetivo

Este documento tem por objetivo principal mostrar alguns dos recursos mais importante da linguagem R, além de apresentar uma análise de dados obtendo informações via web scraping ou coleta de dados web e aplicando técnicas estatísticas.

## Macro Etapas

1.  Captura de dados
2.  Limpeza e transformação
3.  Análise descritiva
4.  Análise de correlação
5.  Conclusão

```{r pasta de trabalho, message=TRUE, warning=TRUE, include=FALSE}
# DEFININDO PASTA DE TRABALHO
setwd("C:/Users/icaro/Documents/Meu_Portfolio/Preco.Combustivel")
getwd()
```

Vamos carregar os pacotes necessários para esta análise.

```{r pacotes, echo=TRUE, message=FALSE, warning=FALSE}
# CARREGANDO PACOTES
library(rvest) # extração dos dados na web
library(tidyverse) # manipulação de dataset
library(lubridate) # manipulação de datas
library(DescTools) # análise descritiva
library(psych) # análise de correlação
```

## Sobre os dados

Escolhemos o site: <https://www.tabelasdefrete.com.br>. Estamos interessados em realizar uma análise de preço dos combustíveis e aqui encontraremos os dados dispostos mensalmente de julho de 2001 a dezembro de 2015.

## Extração dados

Nesta seção, usaremos as funções do primeiro pacote carregado *rvest*. Através da função `read_html()` vamos fazer a leitura da homepage.

```{r Carga, echo=TRUE, message=FALSE, warning=FALSE}
# CARREGANDO PAGINA DA WEB
webpage = read_html("https://www.tabelasdefrete.com.br/planilha/historico-da-variacao-de-precos/25")
```

Com as funções `html_nodes()` e `html_table()` conseguimos chegar num objeto do tipo lista e a partir dele chegamos na tabela de interesse.

```{r Extração}
# EXTRAINDO AS TABELAS CONTIDAS NA PÁGINA
tabelas = 
  webpage %>% 
  html_nodes('table') %>% 
  html_table()

# CLASSE DO OBJETO
class(tabelas)

# GRAVANDO A PRIMEIRA TABELA EM UM OBJETO
dados = tabelas[[1]]

# CABEÇA DO DATASET
head(dados)
```

## Limpeza e transformação dos dados

Como podemos perceber, precisamos realizar algumas alterações no conjunto de dados, pois há muito ruido no dataset. Primeiro, renomearemos as varíaveis com a função `rename()` do pacote *dplyr* e em seguida excluiremos as linhas do cabeçalho que não usaremos.

```{r renomear, echo=TRUE, message=FALSE, warning=FALSE}
# RENOMEANDO OS CAMPOS
dados = 
  dados %>% 
  rename(
    mes = X1,
    gasolina = X2,
    variacao_gas = X3,
    etanol = X4,
    variacao_eta = X5,
    diesel = X6,
    variacao_die = X7,
    diesel_s10 = X8,
    variacao_die_s10 = X9,
    gas_natural = X10,
    variacao_gas_nat = X11
    )

# EXCLUINDO AS PRIMEIRAS LINHAS (SUJEIRA)
dados = dados[-c(1:3),]
```

Os números estão com separador casas decimais a vírgula, O R tem como separador o ponto. Para isso precisamos fazer uma substituição de vírgula para ponto. A função `str_replace_all()` do pacote *stringr* irá nos ajudar.

```{r substituicao, echo=TRUE, message=FALSE, warning=FALSE}
# SUBSTITUINDO VÍRGULA POR PONTO
dados$gasolina = str_replace_all(dados$gasolina, ",", ".")
dados$etanol = str_replace_all(dados$etanol, ",", ".")
dados$diesel = str_replace_all(dados$diesel, ",", ".")
dados$diesel_s10 = str_replace_all(dados$diesel_s10, ",", ".")
dados$gas_natural = str_replace_all(dados$gas_natural, ",", ".")
```

A função `glimpse()` do pacote *dplyr* nos ajuda conhecer melhor no nosso conjunto de dados.

```{r glimpse, echo=TRUE, message=FALSE, warning=FALSE}
# DATASET
glimpse(dados)
```

Então temos `r nrow(dados)` registros e `r ncol(dados)` variáveis. Porém todas variáveis estão como tipo carácter e temos campos que são de outros tipos (data, numérico, etc.). Vamos promover essas alterações.

No pacote *dplyr* temos uma funções `select()`, `separate()` e `mutate()`, usaremos-as para selecionar as variáveis de interesse do estudo, quebrar o campo "mês" em dois novos e transformar o preço em tipo numérico, respectivamente.

```{r transformacao, echo=TRUE, message=FALSE, warning=FALSE}
# SELECIONANDO CAMPOS E ALTERANDO FORMATO
dados = 
  dados %>% 
  select(mes, 
         gasolina,
         etanol,
         diesel,
         gas_natural) %>% 
  separate(mes, c("mes", "ano"), sep = "/") %>% 
  mutate(gasolina = as.numeric(gasolina),
         etanol = as.numeric(etanol),
         diesel = as.numeric(diesel),
         gas_natural = as.numeric(gas_natural))
```

Agora que temos o campo mês quebrado em mês e ano, vamos criar uma variável do tipo data, pois irá nos facilitar o plot gráfico dos dados e melhorar a experíencia de análise.

Para isso, vamos construir um novo conjunto de dados com a função `tibble()` do pacote *tidyverse*.

```{r calendario, echo=TRUE, message=FALSE, warning=FALSE}
# TABELA AUXILIAR
calendario = 
  tibble(
    mes = c('jan', 'fev', 'mar', 'abr', 'mai', 'jun', 'jul', 'ago', 'set', 'out', 'nov', 'dez'),
    mes_num = as.character(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12)))
```


A função `left_join()` levará o número do mês para o nosso conjunto.


```{r join, echo=TRUE, message=FALSE, warning=FALSE}
# CRUZAR AS TABELAS
dados = left_join(dados, calendario, by = "mes")
```


Criando a variável data no formato dia/mês/ano.


```{r var data, echo=TRUE, message=FALSE, warning=FALSE}
# CRIANDO A VARIÁVEL DATA
dados$data = paste("1", dados$mes_num, dados$ano, sep = "/")
```


Precisamos alterar a nova variável para o tipo data conforme a localizada do SO instalado. Para isso, a função `Sys.getlocale()` retorna a localização.


```{r localizacao, echo=TRUE, message=FALSE, warning=FALSE}
# EM QUAL LOCALIZAÇÃO ESTOU PARA O R
Sys.getlocale(category = "LC_TIME")
```

```{r alterar data, echo=TRUE, message=FALSE, warning=FALSE}
# ALTERANDO TIPO DO CAMPO DATA
dados$data = dmy(dados$data, locale = "Portuguese_Brazil.1252")
```


Após as limpezas e transformações realizadas, a última ação necessária para darmos continuidade ao estudo é análise de dados faltantes ou missings. A função `PlotMiss()` do pacote *DescTools* retorna um plot da quantidade de dados faltantes por variável e o percentual em relação ao todo.


```{r valorNA, echo=TRUE, message=FALSE, warning=FALSE}
PlotMiss(dados, main = "Plot de dados faltantes")
```

O gráfico retornou 5 registros com dados faltantes na variável *Gás Natural*. A nossa tratativa em relação a este caso será substituir esses valores por 50% da média da variável, pois sabemos que são dados distribuição ao longo do tempo e assim manteremos um arranjo coerente. Portanto, usaremos a função `replace_na()` do pacote *dplyr* para executar essa ação.


```{r substituicaoNA, echo=TRUE, message=FALSE, warning=FALSE}
# PREENCHENDO OS VALOR NA COM A MÉDIA DO CAMPO, NO CASO GÁS NATURAL
dados$gas_natural = replace_na(dados$gas_natural, mean(dados$gas_natural, na.rm = T) * 0.5)
```




## Análise descritiva



Nossa análise descritiva vai começar com um resumo completo de cada variáveil. O objetivo é extrair alguns insights dessa etapa importante de análise exploratória. A função escolhida para isso é `Desc()` do pacote *DescTools*.


```{r descritiva, , echo=TRUE, message=FALSE, warning=FALSE}
# DESCRITIVA
Desc(dados[, c(3:6)])
```


Esta função é extremamente útil e completa, pois ela resumi o conjunto e apresenta todas as variáveis com gráficos e estatísticas.


Falando do conjunto, temos 173 registros e 4 variáveis. Ocultamos o campo data, para focarmos apenas no preço dos combustíveis. Caso o dataset apresente dados faltantes, o output da função informa quantidade e percentual do todo.


Para cada variável numérica temos informações tais como: os 5 maiores/menores valores, média, mediana, percentis, assimetria, coeficiente de variação, etc.


Temos também os gráficos que apresentam a distribuição dos preços de cada combustível. Neles já podemos observar que a gasolina e o diesel apresentam valores discrepantes, mas todos tem uma curva cuja média e mediana não se distanciam muito, isso é um indicativo de assimetria.


Podemos resumir as medidas estatísticas em um tibble e construir um quadro de comparação. Com os pacotes *knitr* e *kableExtra* vamos formatar o quadro e assim podemos analisar o comportamento dos preços.

```{r quadro, message=FALSE, warning=FALSE, include=FALSE}
# TABELA DE ESTATÍSTICAS
estatisticas = 
  tibble(combustivel = c('Gasolina', 'Etanol', 'Diesel', 'GNV'), 
         media = c(mean(dados$gasolina), mean(dados$etanol), 
                   mean(dados$diesel), mean(dados$gas_natural, na.rm = T)),
         mediana = c(median(dados$gasolina), median(dados$etanol), 
                     median(dados$diesel), median(dados$gas_natural, na.rm = T)), 
         desvio = c(sd(dados$gasolina), sd(dados$etanol), 
                    sd(dados$diesel), sd(dados$gas_natural, na.rm = T)), 
         interquartil = c(IQRw(dados$gasolina), IQRw(dados$etanol), 
                          IQRw(dados$diesel), IQRw(dados$gas_natural, na.rm = T)), 
         coefvariacao = c(CoefVar(dados$gasolina), CoefVar(dados$etanol), 
                          CoefVar(dados$diesel), CoefVar(dados$gas_natural, na.rm = T)),
         assimetria = c(Skew(dados$gasolina), Skew(dados$etanol), 
                        Skew(dados$diesel), Skew(dados$gas_natural, na.rm = T)),
         curtoses = c(Kurt(dados$gasolina), Kurt(dados$etanol), 
                      Kurt(dados$diesel), Kurt(dados$gas_natural, na.rm = T)))
```

```{r kable, echo=TRUE, message=FALSE, warning=FALSE}
# TABELA DE ESTATÍSTICAS
estatisticas %>% 
  knitr::kable(digits = 3, 
               col.names = c("Combustível", "Média", "Mediana", "Desvio-padrão", 
                             "Inter-quartil", "Coef. de variação", "Assimetria", "Curtose"), 
               caption = "Fonte: Tabela de Frete",
               align = 'c') %>% 
  kableExtra::kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
```

Nesta tabela podemos destacar que todos os combustíveis tem médias e medianas bem próximas, GNV tem a menor variação de preços ao longo do período. A gasolina é o combustível mais caro. Quanto ao coeficiente de variação, todos estão entre 0,15 e 0,30, isso significa média dispersão dos preços. Também apresentaram coeficiente de assimetria abaixo de 0,15, portanto podemos considerar distribuições praticamente simétricas. O coeficiente de curtose classifica a distribuição em relação a grau de achatamento da curva e no nosso estudo, todos tiveram valores inferiores a 0,263, portanto as distribuições são leptocúrticas.

Observação: conhecer as características das distribuições auxilia na tomada de decisão de escolha do parâmetro central que melhor representa a variável. No nosso caso as médias estão próximas das medianas, isso é uma evidência que a média é um bom estimador, pois sabemos que sofrem bastante com a presença de outliers (valores discrepantes).



Partiremos para uma análise gráfica dos dados, primeiramente olhando para a série histórica dos preços por cada um dos combustíveis. Iremos fazer uso da função `ggplot()` do pacote *ggplot2*, explorando seus recursos de formatação do gráfico.


```{r gasolina, echo=TRUE, message=FALSE, warning=FALSE}
# GASOLINA
ggplot(dados) +
  aes(x = data, y = gasolina) +
  geom_line(size = 1.3, colour = "#6A8ED0") +
  labs(x = "",
       y = "Preço Gasolina R$/L", 
       title = "Série Histórica - Preço da Gasolina", 
       caption = "Fonte: Tabela de Frete") +
  theme_gray() +
  theme(plot.title = element_text(size = 15L, face = "bold"))
```

```{r alcool, echo=TRUE, message=FALSE, warning=FALSE}
# ETANOL
ggplot(dados) +
  aes(x = data, y = etanol) +
  geom_line(size = 1.3, colour = "#6A8ED0") +
  labs(x = "",
       y = "Preço Etanol R$/L", 
       title = "Série Histórica - Preço do Etanol", 
       caption = "Fonte: Tabela de Frete") +
  theme_gray() +
  theme(plot.title = element_text(size = 15L, face = "bold"))
```

```{r diesel, echo=TRUE, message=FALSE, warning=FALSE}
# DIESEL
ggplot(dados) +
  aes(x = data, y = diesel) +
  geom_line(size = 1.3, colour = "#6A8ED0") +
  labs(x = "",
       y = "Preço Diesel R$/L", 
       title = "Série Histórica - Preço do Diesel", 
       caption = "Fonte: Tabela de Frete") +
  theme_gray() +
  theme(plot.title = element_text(size = 15L, face = "bold"))
```

```{r gnv, echo=TRUE, message=FALSE, warning=FALSE}
# GAS NATURAL
ggplot(dados) +
  aes(x = data, y = gas_natural) +
  geom_line(size = 1.3, colour = "#6A8ED0") +
  labs(x = "",
       y = "Preço GNV R$/L", 
       title = "Série Histórica - Preço do GNV", 
       caption = "Fonte: Tabela de Frete") +
  theme_gray() +
  theme(plot.title = element_text(size = 15L, face = "bold"))
```

Podemos destacar que todas as séries são não-estacionárias, pois há uma forte tendência de crescimento ao longo do tempo.



## Análise de correlação



Uma boa análise estatística que podemos fazer com os dados é verificar o quanto o preço de um combustível está correlacionado a outro. Será que o Diesel tem "influência" no preço do GNV?


Sabemos que o transporte dos combustíveis no Brasil é feito por caminhões, isso já basta para dizermos que o preço do Etanol, por exemplo, leva em consideração os custos de transporte (Diesel). Mas o quanto é correlacionado? Podemos medir isso? Sim, podemos!

Através da função `ggpairs()` do pacote *GGally* podemos construir uma matriz de correlação. Ela é bacana, pois informa além do coeficiente de correlação entre duas variáveis, se o mesmo tem significância estatística. O que isso quer dizer? Bom, após calcularmos o coeficiente, podemos realizar um teste de hipótese em que colocamos como hipótese a ser testada, se o coeficiente é igual a zero e portanto indicando que as variáveis não são correlacionadas.


```{r matriz corr, echo=TRUE, message=FALSE, warning=FALSE}
dados %>% 
  select(gasolina, etanol, diesel, gas_natural) %>% 
  GGally::ggpairs(
    title = "Análise de correlação e dispersão do preço dos combustíveis",
    columnLabels = c("Gasolina", "Etanol", "Diesel", "GNV"))
```


No diagrama resultante, obtivemos os seguintes pontos observáveis: todos os combustíveis apresentaram o coeficiente de correlação acima de 0,8, logo podemos afirmar que são altamente correlacionados. Além disso o R acrescenta o resultado do teste de hipótese. Os asteriscos "***" ao lado do coeficiente, informa que o teste apresentou significância estatística, o mesmo dizer que o p-valor foi abaixo 0,0001. Portanto, podemos afirmar que há evidência de que as variáveis são correlacionadas, ou seja, que podemos rejeitar a hipótese nula (H0: pho = 0). O diagrama apresenta também gráficos de dispersão e histograma das variáveis.



### Regressão Linear


Por meio dos gráficos de dispersão acima, observamos uma relação linear entre as variáveis, o que significa que uma equação de reta pode descrever os dados e auxiliar na interpretação da relação entre elas. Nosso interesse aqui é saber o quanto o preço do Diesel impacta o preço da Gasolina. 


A regressão linear é uma técnica estatística que encontra uma equação que minimize os erros através do método dos mínimos quadrados, utilizamos para descrever a relação linear entre duas variáveis, além de realizar previsões com base na amostra coletada.


Uma análise de regressão requer uma verificação dos pressupostos para que tenhamos resultados não viesados.


1. Relação linear entre as variáveis.
2. Normalidade: os resíduos(erros) seguem uma distribuição normal?
3. Homocedasticidade ou variância constante.
4. Ausência de outliers.
5. Independência dos resíduos.


Primeiramente vamos construir o modelo atráves da função `lm()` do pacote *stats*, informando que queremos como variável preditora ou explicativa (X = diesel) e variável resposta (Y = gasolina). O modelo proposto tem a forma: $\hat y_i$ = $\beta_0$ + $\beta_1$$x_i$, para i variando de 1 até n (tamanho da amostra).


```{r modelo, echo=TRUE, message=FALSE, warning=FALSE}
# CONSTRUIR O MODELO
modelo = lm(formula = gasolina ~ diesel, data = dados)
```

Para o primeiro ponto, vimos nos gráficos de dispersão que há uma relação linear entre as variáveis (gasolina e diesel). O modelo proposto gera resíduos, que são os erros de estimação da reta, que precisamos analisá-los para validar ou não a equação construída. Essa etapa vamos olhar para 4 gráficos: Résiduos vs Valores Ajustados (1), Quantis dos Resíduos Padronizados (2), Raiz dos Resíduos Padrozinados vs Valores Ajustados (3) e Resíduos Padronizados vs alavancagem (4).


```{r residuos, echo=TRUE, message=FALSE, warning=FALSE}
# ANÁLISE DE RESÍDUOS
par(mfrow=c(2,2))
plot(modelo)
```

```{r vizu, message=FALSE, warning=FALSE, include=FALSE}
par(mfrow=c(1,1))
```

(1) O primeiro gráfico (canto superior esquerdo) serve para analisarmos se há relação de linearidade. Temos que a linha vermelha flutua sobre a linha pontilhada indicando relação linear entre as variáveis.


(2) Próximo gráfico (canto superior direito) é utilizado para verificar se os resíduos tem distribuição normal, para isso os pontos devem estar sobre a linha pontilhada. No nosso caso, há alguns pontos bastante distante da linha, o que pode ser uma evidência de não normalidade dos resíduos. Mais a frente vamos analisar com mais profundidade a questão.


(3) Gráfico do canto inferior direito. Este nos auxilia quanto a homocedasticidade, pois em caso de haver um padrão na distribuição dos resíduos pode nos levar a entender que não há variância constante e isso prejudicaria nossas estimativas futuras com estes dados. No nosso caso. a linha vermelha flutua sobre a pontilhada e apresenta uma forma aproximadamente horizontal, indicando haver homocedasticidade, apesar da distribuição dos pontos não se apresentar dispersos de forma homogênea.


(4) E por último, temos o gráfico que serve para identificarmos pontos discrepantes nos resíduos. Caso haja, observaremos pontos além do intervalo -3 e +3 no eixo y. No nosso caso, não temos outliers.


Seguindo nossa análise de pressupostos. Vamos refinar nossa análise da distribuição dos resíduos. NO diagrama anterior, plot QQ-Normal, vimos que a distribuição apresentação muitos pontos que não estão em cima da linha. Um histograma e um teste estatístico são ótimas formas de avaliação.


```{r normal residuos, echo=TRUE, message=FALSE, warning=FALSE}
# TESTE DE NORMALIDADE DOS RESÍDUOS
# H0: OS DADOS SEGUEM UMA DISTRIBUIÇÃO NORMAL
shapiro.test(modelo$residuals)

# HISTOGRAMA DOS RESÍDUOS DO MODELO
res = modelo$residuals
residuos = tibble(residuos = res)
ggplot(residuos) + 
  aes(x = residuos) + 
  geom_histogram(col = "black", 
                 fill = "dark green",
                 alpha = 0.6,
                 bins = 30,
                 aes(y = ..density..)) + 
  stat_function(fun = dnorm, 
                args = list(mean = mean(residuos$residuos), 
                            sd = sd(residuos$residuos))) +
  labs(
    title = "Resíduos do Modelo de Regressão Linear Simples",
    subtitle = "Impacto do diesel no preço da gasolina",
    x = "Resíduos",
    y = "Densidade",
    caption = "Fonte: Tabela de Frete")
```


O teste escolhido foi Shapito Wilk, que tem como hipótese testada que os dados seguem uma distribuição normal. Tivemos a estatística W = 0,98 e um p-valor menor que 5%, portanto, temos uma evidência de não normalidade. Além disso, o histograma tem as caudas longas e a curva está bastante achatada, ou seja, não se assemelha a um sino como esperado, mais uma evidência da falta de normalidade.


As funções `summary()` e `rstandard()` trarão para nós um resumo dos resíduos padronizados.

```{r outliers, echo=TRUE, message=FALSE, warning=FALSE}
# OUTLIERS NOS RESÍDUOS PADRONIZADOS
summary(rstandard(modelo))
```

Outra forma de diagnosticar outliers se dá pelos valores de máximo e mínimo. A literatura diz que os resíduos precisam ter valores entre -3 e +3, pois fora desse intervalo indica presença de outliers. Como vimos, esse pressuposto foi atendido.


É importante que os resíduos não apresente autocorrelação, ou seja, esperamos que sejam independentes. Para isso temos no pacote *DescTools*, a função `DurbinWatsonTest()`. O teste de Durbin-Watson tem como hipótese nula, H0: Os resíduos são independentes. A estatística DW esperada como resultado é um valor próximo de 2.


```{r indep. residuos, echo=TRUE, message=FALSE, warning=FALSE}
# INDEPENDENCIA DOS RESÍDUOS (DURBIN-WATSON)
# H0: AUTOCORRELAÇÃO DOS RESÍDUOS = 0
DescTools::DurbinWatsonTest(modelo)
```

Como podemos ver, o pressuposto não foi atendido. A estatística DW está muito longe do esperado e o p-valor do teste indica que devemos rejeitar a hipótese nula, afirmando que os resíduos não são independentes.


Atráves da função `bptest()` do pacote *lmtest*, podemos realizar o teste de homocedasticidade dos resíduos.

```{r homoc, echo=TRUE, message=FALSE, warning=FALSE}
# HOMOCEDASTICIDADE DOS RESÍDUOS (Breusch-Pagan): VARIANCIA CONSTANTE DOS RESÍDUOS.
# H0: HÁ HOMOCEDASTICIDADE NOS RESÍDUOS.
lmtest::bptest(modelo)
```

Com BP = 0,68971 e p-valor maior que 5%, podemos afirmar que há homodecasticidade nos resíduos, logo este pressuposto foi atendido.


Em seguida vamos fazer uma análise dos parâmetros do modelo com a função `summary()`.

```{r resumo modelo, echo=TRUE, message=FALSE, warning=FALSE}
# ANALISE DO MODELO
# TESTE DE INCLINAÇÃO DA RETA, H0: BETA = 0
# TESTE F OU TESTE DE VALIDAÇÃO DO MODELO: MODELO É ADEQUADO PARA OS DADOS
# O TESTE F COMPARA O MODELO COM PREVISOR OU VAR INDEP COM O MODELO SEM A VI, SÓ COM O INTERCEPTO.
# H0: O MODELO NÃO POSSUI FALTA DE AJUSTE (TESTE F).
summary(modelo)
```

Nesta função o output do R nos informa um resumo dos resíduos, os valores de $\beta_0$/$\beta_1$ e os testes de significância, além do R² e estatística F. Estas são informações essenciais para avaliação do modelo proposto. Com base nos resultados obtidos, podemos dizer que o preço do Diesel é estatisticamente significativo e portanto um bom previsor para o preço da gasolina. O teste F mostrou relevância estatística, logo o nosso modelo é válido e melhor que um modelo sem varíavel independente (Diesel). O R² ajustado foi de 0,96, ou seja, o modelo consegue explicar 96% da variabilidade dos preços. E por último, o parâmetro $\beta_1$ foi de 0,86, isso significa que para um acréscimo de uma unidade de real do preço do Diesel está associada a um acréscimo de 0,86 unidade no preço da Gasolina.


Podemos estabelecer um intervalo de confiança para o coeficiente de regressão ou parâmetro $\beta_1$. com a função `confint.lm()` informamos os argumentos: objeto (modelo), parâmetro de interesse e o nível de confiança.

```{r IC, echo=TRUE, message=FALSE, warning=FALSE}
# INTERVALO DE CONFIANÇA PARA O PARÂMETRO BETA (INCLINAÇÃO DA RETA)
confint.lm(object = modelo, parm = 'diesel', level = 0.95)
```

Então para o nosso coeficiente de regressão de 0,86, obtivemos um intervalo de confiança de 95%: ]0,84 ; 0,89[. Significa dizer que para 95 vezes em 100 esperamos que o nosso intervalo de confiança contenha o verdadeiro valor do parâmetro $\beta_1$ da população.


Abaixo temos uma representação gráfica do modelo de regressão linear simples proposto neste estudo.


```{r dispersao, echo=TRUE, message=FALSE, warning=FALSE}
# GRÁFICO DE DISPERSÃO COM A RETA DO MODELO AJUSTADO
# DIESEL VS GASOLINA
ggplot(dados) +
  aes(x = diesel, y = gasolina) +
  geom_point(shape = "circle", size = 1.5, colour = "#6A8ED0") +
  geom_smooth(method = "lm", col = "red") +
  labs(
    title = "Relação de Preços - Diesel x Gasolina",
    subtitle = "Modelo de regressão linear simples",
    x = "Diesel",
    y = "Gasolina",
    caption = "Fonte: Tabela de Frete"
  ) + 
  ggpubr::stat_regline_equation(
    aes(label = paste(..eq.label.., ..adj.rr.label.., sep = "*plain(\",\")~~"))) +
  theme_gray() + 
  theme(plot.title = element_text(face = "bold", size = 15L))
```


## Conclusão


Ao final desta análise, podemos afirmar que a técnica de regressão linear não pode ser aplicada aos dados para se fazer estimativas futuras no preço da gasolina, uma vez que não tivemos todos os pressupostos atendidos. Em especial podemos destacar que, o fato dos resíduos não apresentar independência se dá pelo fato de que os nossos dados estão distribuidos ao longo do tempo. O resultado do teste de Durbin-Watson nos diz que a regressão linear não é adequada para realizar previsões com dados dispostos em série. O melhor a se fazer é aplicar um modelo de séries temporais!


Apresentamos uma análise de dados do começo com a coleta dos dados, passando por limpeza e transformação do conjunto de dados. Desenvolvemos uma análise exploratória dos dados e com técnica estatística estudamos a correlação entre variáveis númericas e apresentamos os resultados. Grosseiramente, esse é o passo-a-passo de uma análise estatística.


Com este trabalho, espero ter passado um pouco do que sei sobre análise de dados com a linguagem R, apresentando os principais recursos do R, bem como o poder que a estatística tem de extrair informações de dados brutos para gerar inteligência e ações que podem impactar nossa sociedade.


## Referências

<https://proeducacional.com/ead/curso-cga-modulo-i/capitulos/capitulo-4/aulas/covariancia-e-correlacao/>

<https://pt.stackoverflow.com/questions/6979/como-colocar-a-equação-da-regressão-em-um-gráfico>

<https://data.library.virginia.edu/diagnostic-plots/>

<https://br.investing.com>

<http://professor.ufop.br/sites/default/files/ericarodrigues/files/regressaolinearsimples_parte1.pdf>

<http://professor.ufop.br/sites/default/files/ericarodrigues/files/regressaolinearsimples_parte2.pdf>

<http://professor.ufop.br/sites/default/files/ericarodrigues/files/regressaolinearsimples_parte4.pdf>

<http://professor.ufop.br/sites/default/files/ericarodrigues/files/regressaolinearsimples_parte5.pdf>
